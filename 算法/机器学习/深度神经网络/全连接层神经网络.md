# 全连接层神经网络

## 简化的两层神经网络

![pic2](.\pic\pic2.jpg)

- 输入层

  假设是一个坐标值$(1,1)$那么这就是一个包含两个元素的数组，或者一个$1*2$的矩阵。

- 输入层到隐藏层
  $$
  H=X*W_1+b_1
  $$
  如图所示隐藏层设定为$50$维，也就是说$H$的结果为$1*50$的矩阵，$W_1$可以是一个$2*50$的矩阵，$1*2$的矩阵和$2*50$的矩阵的计算结果是$1*50$的矩阵，也就是$H$的形状，$b_1$的形状为$1*50$

- 隐藏层到输出层

  $$
  Y=H*W_2+b_2
  $$
  $H$形状为$1*50$的矩阵，$W_2$是一个$50*4$的矩阵，两者相乘的结果为一个$1*4$的矩阵，也就是$Y$和偏移量$b_2$的形状

## 激活层——给神经网络加入非线性

三个激活函数：阶跃函数、sigmoid函数以及ReLU

- 阶跃函数：小于等于0，输出0；大于0，输出1
- sigmoid函数：输入接近无穷的时候，输出接近0 / 1
- ReLU：输入小于0的时候，输出为0；输入大于0的时候，输出等于输入

![pic3](.\pic\pic3.jpg)

对隐藏层使用激活函数，进而对向量进行处理。

## 输出的正规化

输出层Y的四个数值代表四个答案分别的可能性，如果我们要达到分类的目的，就要找到四个结果的最大值，当然直接使用max可能存在不方便反向传播的问题，因此我们采用softmax的方式来解决
$$
S_i=\frac{e^i}{\sum_j e^j}
$$
在加入softmax之后结果变成这样

![pic4](.\pic\pic4.jpg)

## 使用交叉熵来衡量结果的好坏

我们可以这样想softmax的输出结果为(90%,5%,3%,2%)，而真实的结果为(100%,0,0,0)。很明显，这两者之间存在差距，为了衡量这种差距，我们使用交叉熵
$$
H(P,Q)=-\sum_{i=1}^nP(x_i)\log Q(x_i)
$$
使用P（真实概率）计算概率，使用Q（假设概率）计算二进制位数

> 信息论中熵描述了一种理想情况下能够使得编码长度最小的编码方式，也就是说对于一个出现可能性为$p(x)$的信息使用长度为$-\log p(x)$的编码，我们使用$p(x)$乘上$-\log p(x)$加和即可得到平均最小编码长度，这种情况下，平均编码长度是最小的
>
> 但是如果对于$p(x)$我们使用长度为$-\log Q(x)$的编码那么这种情况下其平均编码长度一定相较于$-\log p(x)$更加差，因此我们可以通过交叉熵的大小来衡量理论概率和真实概率之间的差距，只有理论概率和真实概率完全相同，交叉熵才能最小；换言之，减小交叉熵的数值能够增加理论概率和真实概率的相似性

## 仿射变换的求导

我们观察仿射变换的结构

![pic5](.\pic\pic5.jpg)

输入层$X$结构为$1*2$的矩阵，$W_1$为$2*50$的矩阵，$H$和$b_1$为$1*50$的矩阵
$$
H=X*W_1+b_1
$$
我们现在要通过对$W_1$和$b_1$进行修正来使得$H$的值更加小，可以如此操作
$$
\begin{cases}
\frac{\part H}{\part b_1}=1\\
\frac{\part H}{\part W_1}=X
\end{cases}
$$
我们发现$H$对$b_1$求导结果为$1$，$H$对$W_1$求导结果为$X$

## ReLU层的求导

激活层的形式

![pic6](.\pic\pic6.jpg)

当x>0时，y=x，求导为1，也就是原封不动传递。

当x<=0时，y=0，求导为0，也就是传递值为0。

## softmax的求导

softmax函数的表达式
$$
Softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}}
$$
我们令$p_i=Softmax(z_i)$

在$p_i=Softmax(z_i)$关于$z_j$的求导中$j$分为$j\neq i$和$j=i$两种情况

- 关于$j=i$
  $$
  \begin{align}
  \frac{\part}{\part z_i}(\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}})&=\frac{e^{z_i}(\sum_{c=1}^Ce^{z_c})-e^{z_i}e^{z_i}}{(\sum_{c=1}^Ce^{z_c})^2}\\
  &=\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}}(1-\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}})\\
  &=p_i(1-p_i)
  \end{align}
  $$

- 关于$j\neq i$
  $$
  \begin{align}
  \frac{\part}{\part z_j}(\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}})&=-\frac{e^{z_i}}{(\sum_{c=1}^Ce^{z_c})^2}e^{z_j}\\
  &=-\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}}\frac{e^{z_j}}{\sum_{c=1}^Ce^{z_c}}\\
  &=-p_ip_j
  \end{align}
  $$

因此我们能够认为关于softmax的偏导数表示如下
$$
\frac{\part p_i}{\part z_j}=
\begin{cases}
p_i(1-p_i)&j=i\\
-p_ip_j&j\neq i
\end{cases}
$$

## 交叉熵

### 从softmax到交叉熵

我们观察$p_i=\frac{e^{z_i}}{\sum_{c=1}^Ce^{z_c}}$，对$p_i$取$\log$即为$\log p_i=z_i-\log\sum^C_{c=1}e^{z_c}$

$y_i$被认为是正确分类的概率，那么我们就认为$\log y_i$的值应当是越大越好，当然这和$loss$的定义相背离，因此我们定义$softmax$的损失函数为
$$
loss=-\log p_i=-(z_i-\log\sum_{c=1}^Ce^{z_c})
$$

- loss函数和交叉熵的关系

  我们对于交叉熵的定义一般如下
  $$
  L=-\sum_{c=1}^Cy_c\log p_c
  $$
  也就是真实标签和理论标签的交叉

  这种情况下，只有正确的标签为1，其余皆为0，我们将这种情况代入
  $$
  L=-\log p_i=loss
  $$
  由此我们发现softmax的损失函数就是交叉熵

- 如何理解$softmax$

  我们可以这样理解softmax的正向传播，我们现在有一个向量如下
  $$
  \begin{bmatrix}
  z_1\\
  z_2\\
  \ldots\\
  z_n
  \end{bmatrix}
  $$
  我们对$z_i$做如下计算
  $$
  softmax(z_i)=\frac{e^{z_i}}{\sum_{c=1}^ne^{z_c}}
  $$
  经过$softmax$运算之后得到一个如下的向量
  $$
  \begin{bmatrix}
  \frac{e^{z_1}}{\sum_{c=1}^ne^{z_c}}\\
  \frac{e^{z_2}}{\sum_{c=1}^ne^{z_c}}\\
  \ldots\\
  \frac{e^{z_n}}{\sum_{c=1}^ne^{z_c}}
  \end{bmatrix}
  $$
  其中假设$j$为正确分类，这种情况下损失函数即为$loss=-\log p_j$

  我们可以这样子认为$loss$是一个关于$z_1,z_2,\ldots,z_n$的函数，我们能够求其关于$z_i$的变化率，进而求得梯度下降矩阵
  $$
  \begin{align}
  \frac{\part L}{\part z_i}&=-\sum_{c=1}^ny_c\frac{\part\log p_c}{\part z_i}\\
  &=-\sum_{c=1}^ny_c\frac{1}{p_c}\frac{\part p_c}{\part z_i}
  \end{align}
  $$
  我们可以将$c$划分为$c=i$和$c\neq i$两种情况
  $$
  \begin{align}
  \frac{\part L}{\part z_i}&=-y_i\frac{1}{p_i}p_i(1-p_i)-\sum_{c=1}^{n,c\ne i}y_c\frac{1}{p_c}(-p_ip_c)\\
  &=-y_i(1-p_i)+\sum_{c=1}^{n,c\ne i}y_cp_i\\
  &=-y_i+p_i
  \end{align}
  $$

因此我们能够认为关于$softmax$的梯度下降矩阵（$c$为正确标签）为
$$
\begin{bmatrix}
p_1\\
p_2\\
\ldots\\
-1+p_c
\\
\ldots\\
p_n
\end{bmatrix}
$$

## 关于softmax的总结

$softmax$是一个多分类激活函数，通过指数函数进行激活

$softmax$使用交叉熵作为$loss$函数，对$loss$函数求关于$z_i$的导数，进而求得$softmax$的梯度下降矩阵

求$loss$函数关于$z_i$的导数的过程中通过提前计算$\frac{\part p_c}{\part z_i}$的结果来简化计算

