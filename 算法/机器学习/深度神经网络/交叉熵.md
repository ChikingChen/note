# 交叉熵

## 熵

平均最小编码长度

- $-\log_2p(x)$——编码当前信息需要的二进制位数
- $p(x)$——当前信息出现的概率

$$
Entropy=-\sum_xp(x)\log_2p(x)
$$

## 交叉熵

$$
H(P,Q)=-\sum_{i=1}^nP(x_i)\log Q(x_i)
$$

使用P（真实概率）计算概率，使用Q（假设概率）计算二进制位数

熵是理论上最小的平均编码长度，交叉熵只可能大于等于熵

## 使用交叉熵来作为损失函数

交叉熵中两个不同的概率分布只有在完全相同的情况下，也就是交叉熵退化为熵的情况下，交叉熵才会最小

信息论中熵描述了一种理想情况下能够使得编码长度最小的编码方式，也就是说对于一个出现可能性为$p(x)$的信息使用长度为$-\log p(x)$的编码，我们使用$p(x)$乘上$-\log p(x)$加和即可得到平均最小编码长度，这种情况下，平均编码长度是最小的

但是如果对于$p(x)$我们使用长度为$-\log Q(x)$的编码那么这种情况下其平均编码长度一定相较于$-\log p(x)$更加差，因此我们可以通过交叉熵的大小来衡量理论概率和真实概率之间的差距，只有理论概率和真实概率完全相同，交叉熵才能最小；换言之，减小交叉熵的数值能够增加理论概率和真实概率的相似性

