# 交叉熵

## 熵

平均最小编码长度

- $-\log_2p(x)$——编码当前信息需要的二进制位数
- $p(x)$——当前信息出现的概率

$$
Entropy=-\sum_xp(x)\log_2p(x)
$$

## 交叉熵

$$
H(P,Q)=-\sum_{i=1}^nP(x_i)\log Q(x_i)
$$

使用P（真实概率）计算概率，使用Q（假设概率）计算二进制位数

熵是理论上最小的平均编码长度，交叉熵只可能大于等于熵

## 使用交叉熵来作为损失函数

交叉熵中两个不同的概率分布只有在完全相同的情况下，也就是交叉熵退化为熵的情况下，交叉熵才会最小