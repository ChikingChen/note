# 全连接层神经网络

- 模型变量

  $z_j^l$第$l$层第$j$个神经元的输入，$z^1_j=x_j$为第一层第$j$个神经元的输入

  $a_j^l$第$l$层第$j$个神经元的输出，$a_j^1=z_j^1=x_j$第一层第$j$个神经元的输入输出都是$x_j$

- 模型参数

  $w_{ji}^l$代表$l-1$层的第$i$个神经元指向$l$层的第$j$个神经元的权重

  $b_j^l$代表第$l$层第$j$个神经元的偏置

- 学习数据

  $k$为学习数据编号，$M$为学习数据总数

- 数学模型

  第$l$层第$j$个神经元的加权输入
  $$
  z_j^l=
  \begin{cases}
  x_j,&\mathbf{z}^l=\mathbf{x}&l=1\\
  \sum_{i=1}^{n^{l-1}}w_{ji}^la_i^{l-1}+b_j^l,&\mathbf{z}^l=\mathbf{W}^l\mathbf{a}^{l-1}+\mathbf{b}^l&l>1
  \end{cases}
  $$
  第$l$层第$j$个神经元的输出
  $$
  a_j^l=
  \begin{cases}
  x_j,&\mathbf{a}^l=\mathbf{x}&l=1\\
  a(z_j^l),&\mathbf{a}^l=a(\mathbf{z}^l)&l>1
  \end{cases}
  $$
  

- 损失函数

  损失函数基本形式
  $$
  C=\sum_{k=1}^MC[k]
  $$

  $$
  \begin{align}
  C[k]&=\frac{1}{2}\sum_{j=1}^{n^N}(t_j[k]-a_j^N[k])^2\\
  &=\frac{1}{2}(\mathbf{t}[k]-\mathbf{a}^N[k])^2
  \end{align}
  $$

  第$k$个训练样本的损失为结果向量减去正确向量平方的总和
  $$
  \begin{align}
  C&=\frac{1}{2}\sum^{n^N}_{j=1}(t_j-a_j^N)^2\\
  &=\frac{1}{2}(\mathbf{t}-\mathbf{a}^N)^2
  \end{align}
  $$
  $n^N$是第$N$层神经元数量，$t_j$代表所有正确结果的向量第$j$个数值组成的行向量，$a_j^N$所有预测结果的向量第$j$个数值组成的行向量

  将所有训练样本的训练结果转化为列向量堆叠成矩阵并与正确结果堆叠成的矩阵相减平方加和即为结果

## 梯度下降算法

![pic1](.\pic\pic1.png)

- $x_1=1$，$x_2=0.5$，$t=4$

- 随机化，$w_1=0.5$，$w_2=1.5$，$w_3=2.3$，$w_4=3$，$w_5=1$，$w_6=1$
  $$
  \begin{align}
  h_1&=w_1\cdot x_1+w_2\cdot x_2=1.25\\
  h_2&=w_3\cdot x_1+w_4\cdot x_2=3.8\\
  y&=w_5\cdot h_1+w_6\cdot h_2=5.05\\
  E&=\frac{1}{2}(y-t)^2=0.55125
  \end{align}
  $$
  计算$w_5$的偏导
  $$
  \begin{align}
  \frac{\part E}{\part w_5}&=\frac{\part E}{\part y}\cdot\frac{\part y}{\part w_5}\\
  &=(y-t)\cdot h_1\\
  &=1.3125
  \end{align}
  $$
  计算$w_1$的偏导
  $$
  \begin{align}
  \frac{\part E}{\part w_1}&=\frac{\part E}{\part y}\cdot\frac{\part y}{\part h_1}\cdot\frac{\part h_1}{\part w_1}\\
  &=(y-t)\cdot w_5\cdot x_1
  \end{align}
  $$
  

