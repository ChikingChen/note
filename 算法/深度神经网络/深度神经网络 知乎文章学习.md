# [深度神经网络](https://zhuanlan.zhihu.com/p/29815081)

## DNN中的具体参数

![pic7](.\pic\pic7.png)

- 线性关系系数$w$的定义，$w_{24}^3$——第2层第4个元素指向第3层第2个元素

  我们直接看第2层到第3层的仿射变换
  $$
  y=wx+b
  $$
  $x$为$4*1$的矩阵，$y$为$2*1$的矩阵，$x$则为$2*4$的矩阵，$2$代表第三层的元素数量，$4$代表第二层的元素数量

![pic8](.\pic\pic8.png)

- 偏置$b$的定义

  $b^2_3$代表第2层第3个元素

## 前向传播的表示

$a^l$为第$l$层的输出，$\sigma$则为激活函数，$z_l$为神经元的计算结果
$$
a^l=\sigma(z^l)=\sigma(W^la^{l-1}+b^l)
$$

## 反向传播算法

我们将神经网络的损失函数定义如下
$$
J(W,b,x,y)=\frac{1}{2}||a^L-y||_2^2
$$
我们需要计算的实际上只有两个公式，分别罗列如下
$$
\begin{align}
\frac{\part J(W,b,x,y)}{\part W^l}&=\frac{\part J(W,b,x,y)}{\part z^l}\frac{\part z^l}{\part W^l}\\
\frac{\part J(W,b,x,y)}{\part b^l}&=\frac{\part J(W,b,x,y)}{\part z^l}\frac{\part z^l}{\part b^l}\\
\end{align}
$$
现在我们将计算任务分别聚焦在三个点上，$\frac{\part J(W,b,x,y)}{\part z^l},\frac{\part z^l}{\part W^l},\frac{\part z^l}{\part b^l}$

对于后面两者实际上计算较简单，我们关注$z^l$的表达式
$$
z^l=W^la^{l-1}+b^l
$$
由此易得
$$
\begin{align}
\frac{\part z^l}{\part W^l}&=a^{l-1}\ \ \ (m_{l-1}\times1)\\
\frac{\part z^l}{\part b^l}&=1\ \ \ \ \ \ \ \ (1\times 1)
\end{align}
$$
实际上我们的问题则关注于$\frac{\part J(W,b,x,y)}{\part z^l}$的计算
$$
\frac{\part J(W,b,x,y)}{\part z^l}=\frac{\part J(W,b,x,y)}{\part z^L}\frac{\part z^L}{\part z^{L-1}}\ldots\frac{\part z^{l+1}}{\part z^l}
$$
那么问题有分为了两部分$\frac{\part J(W,b,x,y)}{\part z^L},\frac{\part z^{k+1}}{\part z^k}$，我们关注$z^{k+1}$的表达式
$$
\begin{align}
z^{k+1}&=W^{k+1}a^k+b^{k+1}\\
&=W^{k+1}\sigma(z^k)+b^{k+1}
\end{align}
$$
那么的话$\frac{\part z^{k+1}}{\part z^k}$的结果是显然的
$$
\delta^k=\frac{\part z^{k+1}}{\part z^k}=(W^{k+1})^T\odot\sigma(z^k)\ \ \ (m_k\times m_{k+1})
$$
然后是$\frac{\part J(W,b,x,y)}{\part z^L}$
$$
\frac{\part J(W,b,x,y)}{\part z^L}=(a^L-y)\odot\sigma^{'}(z^L)\ \ \ (m_L\times1)
$$
最后通过转置调整矩阵的形状使其符合反向传播算法的形状要求

## 用交叉熵+sigmoid损失函数改进DNN算法收敛速度

使用最小二乘法作为损失函数，将会面临收敛速度过慢的问题，我们观察sigmoid函数图像

![pic9](.\pic\pic9.jpg)

在输入数值过高的情况下可能会出现收敛速度过慢的情况，因此均方差损失函数+sigmod激活函数实际上面临收敛速度过慢的问题

