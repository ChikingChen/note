# [GNN](https://distill.pub/2021/gnn-intro/#graph-to-tensor)

使用一个张量来描述图的一个点，多个张量合并为一个形状为$[n_{node},node_{dim}]$的矩阵。用同样的方式我们也能描述图中的边。

## 最简单的GNN

使用深度神经网络来训练GNN，将点向量、边向量以及全图向量放入目标网络进行训练。训练的输出结果相较于输入结果在结构上没有变化，包括向量的维度、向量的种类以及向量的数量等等。

通过对每个结点使用线性分类器来完成结点的二分类任务。一些情况下，可能会出现只有边的信息，但我们要预测结点的信息。我们能够通过池化来完成这个过程。

- 如何完成池化？

  将与点相关的边聚集起来并加和成一个合向量作为点向量

## 信息传递

在消息传递机制中，我们通过GNN层内池化来做出更加复杂的预测，从而使得我们的组件能够了解图的连通性。消息传递机制种，相邻的结点和边将相互交换信息

消息传递机制的过程：将所有相邻的点组件（也就是信息）收集起来加和，将所有池化后的信息放入一个神经网络进行训练

相较于池化只能在结点和结点之间或者边和边之间应用，消息传递在结点和边之间应用

消息传递和卷积实际上比较像，都是为了将当前节点和相邻的结点联系起来

> 消息传递机制
>
> 将与结点相连的边的信息与结点的信息加和在一起进行池化

## 边表示

以上的通过池化将边的信息转化为点的信息，但这种操作只能用于模型最后一步的预测。但是消息传递过程中，点向量和边向量往往结构不同，需要使用函数将两者结构统一起来。

构建GNN的过程中我们能够选择更新什么图的属性以及以哪个顺序更新，我们可以选择节点组件和边组件哪者先更新。

更新方式实际上分为四种：节点到节点，边到边，结点到边，边到结点。将这四种结合构成**编织**的形式。

> GNN的池化操作
>
> 我们能够理解的是GNN所研究的图实际上是一个整体，这和CNN研究的图像实际上非常类似，当我们要研究一个结点的性质的时候不能只关注于这个结点，而应该关注与这个结点相连的边和结点，因为这个连接性实际上也能够反应结点的性质。这种操作在GNN中被称为消息传递，区别于普通的池化。我们能够理解的是普通的池化的目标是结构相同的向量，但是在消息传递中我们需要在结点向量和边向量之间进行池化操作，这就牵涉到边向量到结点向量的方程转化以及姐点向量到边向量的方程转化。

## 全局表示

如果我们构建了一个k层的网络，那么在上面这种方式中一个节点最多能够接受距离其k结点的影响，但是如果我们希望这个结点接受整个网络的信息，这样做就没有意义了。实际上面对一些比较小的网络的时候我们能够通过引入添加虚边的方式来解决这个问题，但面对一些较大的网络的时候我们就要引入一个代表全图的结点来解决。

这个全局结点能够连接图中所有的结点和所有的边，通过这个结点，我们能够表示整个图的属性。