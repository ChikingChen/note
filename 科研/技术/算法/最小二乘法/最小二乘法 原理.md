# [最小二乘法 解释](https://blog.csdn.net/MoreAction_/article/details/106443383)

## 最小二乘法

$$
L=\sum_{i=1}^n(y_i-f(x_i))^2
$$

## 最小二乘法的背景

提出者认为：让误差平方和最小就能够达到真实状态。

基于的结论：观察值的误差服从标准正态分布，$\epsilon\sim N(0,\sigma^2)$。

我们作如下定义，真实模型参数为$\theta$，模型真实输出为$f_{\theta}(x_i)$，误差服从正态分布$\epsilon\sim N(0,\sigma^2)$，

- $\theta$是什么意义

  我们能够理解的是，一个模型中有众多参数，例如深度神经网络中每个神经元的连接都有一个权重，每个神经元都有一个偏置值，这个$\theta$抽象地代表了这里的每一个值

对于一份数据$(x_i,y_i)$，其发生的概率如下表示
$$
p(y_i|x_i;\theta)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-f_{\theta(x_i)})^2}{2\sigma^2}}
$$
我们能够写出其似然函数
$$
L(\theta)=\prod_{i=1}^mp(y_i|x_i;\theta)=\prod_{i=1}^m\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-f_{\theta(x_i)})^2}{2\sigma^2}}
$$
取$\log$即为
$$
\begin{align}
J(\theta)&=\log(L(\theta))\\
&=\sum_{i=1}^m\log p(y_i|x_i;\theta)\\
&=\sum_{i=1}^m\log(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-f_{\theta(x_i)})^2}{2\sigma^2}})\\
&=-\frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-f_{\theta}(x_i))^2-m\log\sigma\sqrt{2\pi}
\end{align}
$$
我们发现最大化$J(\theta)$实际上就是最小化$\sum_{i=1}^n(y_i-f_{\theta}(x_i))^2$

- 如何理解？

  最大似然估计计算的本质是通过给出的观测数据来推测参数，我们要找到$\theta$最有可能出现的位置，也就是说我们要找到一个$\theta$使得观测数据尽可能成立，实际上这就是我们希望通过算法降低损失函数的目的所在，而我们观察$J(\theta)$极大似然函数，发现增大$J(\theta)$实际上就是在降低$\sum_{i=1}^n(y_i-f_{\theta}(x_i))^2$