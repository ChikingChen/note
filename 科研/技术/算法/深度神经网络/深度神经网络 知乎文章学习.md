# [深度神经网络](https://zhuanlan.zhihu.com/p/29815081)

## DNN中的具体参数

![pic7](.\pic\pic7.png)

- 线性关系系数$w$的定义，$w_{24}^3$——第2层第4个元素指向第3层第2个元素

  我们直接看第2层到第3层的仿射变换
  $$
  y=wx+b
  $$
  $x$为$4*1$的矩阵，$y$为$2*1$的矩阵，$x$则为$2*4$的矩阵，$2$代表第三层的元素数量，$4$代表第二层的元素数量

![pic8](.\pic\pic8.png)

- 偏置$b$的定义

  $b^2_3$代表第2层第3个元素

## 前向传播的表示

$a^l$为第$l$层的输出，$\sigma$则为激活函数，$z_l$为神经元的计算结果
$$
a^l=\sigma(z^l)=\sigma(W^la^{l-1}+b^l)
$$

## 反向传播算法

我们将神经网络的损失函数定义如下
$$
J(W,b,x,y)=\frac{1}{2}||a^L-y||_2^2
$$
我们需要计算的实际上只有两个公式，分别罗列如下
$$
\begin{align}
\frac{\part J(W,b,x,y)}{\part W^l}&=\frac{\part J(W,b,x,y)}{\part z^l}\frac{\part z^l}{\part W^l}\\
\frac{\part J(W,b,x,y)}{\part b^l}&=\frac{\part J(W,b,x,y)}{\part z^l}\frac{\part z^l}{\part b^l}\\
\end{align}
$$
现在我们将计算任务分别聚焦在三个点上，$\frac{\part J(W,b,x,y)}{\part z^l},\frac{\part z^l}{\part W^l},\frac{\part z^l}{\part b^l}$

对于后面两者实际上计算较简单，我们关注$z^l$的表达式
$$
z^l=W^la^{l-1}+b^l
$$
由此易得
$$
\begin{align}
\frac{\part z^l}{\part W^l}&=a^{l-1}\ \ \ (m_{l-1}\times1)\\
\frac{\part z^l}{\part b^l}&=1\ \ \ \ \ \ \ \ (1\times 1)
\end{align}
$$
实际上我们的问题则关注于$\frac{\part J(W,b,x,y)}{\part z^l}$的计算
$$
\frac{\part J(W,b,x,y)}{\part z^l}=\frac{\part J(W,b,x,y)}{\part z^L}\frac{\part z^L}{\part z^{L-1}}\ldots\frac{\part z^{l+1}}{\part z^l}
$$
那么问题有分为了两部分$\frac{\part J(W,b,x,y)}{\part z^L},\frac{\part z^{k+1}}{\part z^k}$，我们关注$z^{k+1}$的表达式
$$
\begin{align}
z^{k+1}&=W^{k+1}a^k+b^{k+1}\\
&=W^{k+1}\sigma(z^k)+b^{k+1}
\end{align}
$$
那么的话$\frac{\part z^{k+1}}{\part z^k}$的结果是显然的
$$
\delta^k=\frac{\part z^{k+1}}{\part z^k}=(W^{k+1})^T\odot\sigma(z^k)\ \ \ (m_k\times m_{k+1})
$$
然后是$\frac{\part J(W,b,x,y)}{\part z^L}$
$$
\frac{\part J(W,b,x,y)}{\part z^L}=(a^L-y)\odot\sigma^{'}(z^L)\ \ \ (m_L\times1)
$$
最后通过转置调整矩阵的形状使其符合反向传播算法的形状要求

## 用交叉熵+sigmoid损失函数改进DNN算法收敛速度

使用均方差作为损失函数，将会面临收敛速度过慢的问题，我们观察sigmoid函数图像

![pic9](.\pic\pic9.jpg)

在输入数值过高的情况下可能会出现收敛速度过慢的情况，其次我们观察对$z^L$的求导情况
$$
\frac{\part J(W,b,x,y)}{\part z^L}=(a^L-y)\odot\sigma^{'}(z^L)
$$
我们发现因为结果中乘了一个$\sigma^{'}(z^L)$，而$\sigma^{'}(z^L)$在$z^L$数值较大的时候，导数较小，将会导致模型输出层收敛较慢

另外，因为$\frac{\part J(W,b,x,y)}{\part z^L}$是链式求导的基础，因此导致前面每一层收敛都极慢

### 二分类交叉熵损失函数

$$
J(W,b,a,y)=-[y\ln a+(1-y)\ln(1-a)]
$$

详细解释查看交叉熵的定义，我们对这个交叉熵损失函数求导
$$
\frac{\part J(W,b,a^L,y)}{\part z^L}=a^L-y
$$
对比均方差损失函数的求导
$$
\frac{\part J(W,b,x,y)}{\part z^L}=(a^L-y)\odot\sigma^{'}(z^L)
$$
我们发现$\sigma^{'}(z^L)$这个导致均方差+$sigmoid$收敛减慢的原因在交叉熵+$sigmoid$方案中没有体现，因此我们能够认为交叉熵+$sigmoid$方案解决了收敛慢的问题

因此我们能够推测的是如果我们使用了$sigmoid$激活函数，搭配交叉熵损失函数往往效果更好

## 用对数似然损失函数和softmax激活函数进行分类输出

$softmax$激活函数
$$
a_i^L=\frac{e^{z_i^L}}{\sum_{j=1}^{n_L}e^{z_j^L}}
$$
我们使用$a_i^L$来表示预测第$i$种样本的概率

我们对正确类别的概率取$ln$并且取负即可得到$softmax$损失函数
$$
J(W,b,a^L,y)=-\ln a_i^L
$$

## [梯度消失和梯度爆炸](https://zhuanlan.zhihu.com/p/33006526)

我们考虑我们的矩阵求导公式
$$
\frac{\part J(W,b,x,y)}{\part z^l}=\frac{\part J(W,b,x,y)}{\part z^L}\frac{\part z^L}{\part z^{L-1}}\ldots\frac{\part z^{l+1}}{\part z^l}
$$
如果我们连乘了一大串大于1的数字，那么我们的数值就会过大，导致梯度爆炸；如果我们连乘了一大串小于1的数字，那么我们的数值就会过小，导致梯度消失。目前梯度消失没有完美解决的办法，梯度爆炸可以通过调整初始化参数解决。

- 梯度爆炸的原因

  模型初始化参数过大，导致数值普遍偏大造成梯度爆炸

- 使用ReLU函数来解决梯度消失的问题

  ReLU函数解决梯度消失
  $$
  \sigma(z)=max(0,z)
  $$
  如果激活函数为1，那么就不用解决梯度爆炸和消失的问题

