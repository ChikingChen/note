# 并行化与增量化图神经网络

研究如何并行化图神经网络以提高效率和可扩展性：

**项目一**‌：研究如何将不同的并行执行模型集成到一个单一、自适应且统一的模型中，以提高效率和可扩展性，同时减轻用户寻找最优模型的工作负担。可以考虑之前的工作，包括*Adaptive Asynchronous Parallelization of Graph Algorithms*和*Graph Computation with Adaptive Granularity*，并尝试将它们从传统的图分析背景迁移到图学习背景中。

- Seed paper:
  - Shao Y, Li H, Gu X, et al. Distributed graph neural network training: A survey[J]. ACM Computing Surveys, 2024, 56(8): 1-39.
  - Besta M, Hoefler T. Parallel and distributed graph neural networks: An in-depth concurrency analysis[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024.

**项目二**‌：研究如何在确保效率的同时提高增量/时序图神经网络训练的准确性。对于动态/时序图，随时间保持一致性具有挑战性，因为模型需要确保更新不会使之前学习的模式失效或引入不一致性。一致性方面的典型挑战包括概念漂移、类别不平衡、偏向于最近数据、灾难性遗忘等。众所周知，在动态/时序图上的图学习与在静态图上的图学习相比，在准确性方面存在差距。一种可能的解决方案是将我们在图分析的增量化方面所取得的成果迁移到图学习中。关键在于建立关于结果质量和效率的理论结果：在质量方面，我们能否确保增量计算与从头训练模型一样好？在效率方面，我们能否确保增量有界性或相对有界性？

- Seed paper:
  - Longa A, Lachi V, Santin G, et al. Graph neural networks for temporal graphs: State of the art, open challenges, and opportunities[J]. arXiv preprint arXiv:2302.01018, 2023.
  - Wenfei Fan, Muyang Liu, Chao Tian, Ruiqi Xu, and Jingren Zhou, Incrementalization of Graph Partitioning Algorithms The 46th International Conference on Very Large Data Bases (VLDB ), 2020.
  - Wenfei Fan, Chao Tian, Qiang Yin, Ruiqi Xu, Wenyuan Yu, and Jingren Zhou., Incrementalizing Graph Algorithms, ACM SIGMOD Conference on Management of Data (SIGMOD), 2021.